{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Arvest batch media upload](https://raw.githubusercontent.com/arvest-data-in-context/ml-notebooks/refs/heads/main/docs/images/notebooks/arvest-batch-media-upload.png)\n",
    "\n",
    "In this notebook, we'll take either local or online media, and see how to upload it to [Arvest](https://arvest.app) using the [Arvest API](https://github.com/arvest-data-in-context/arvest-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Let's begin by installing and importing all of the different components we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing and importing packages...\")\n",
    "\n",
    "# Uninstall and reinstall packages for a clean environment\n",
    "!pip uninstall -q -y arvestapi\n",
    "!pip uninstall -q -y jhutils\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/arvest-data-in-context/arvest-api.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/jdchart/jh-py-utils.git\n",
    "\n",
    "# Import packages\n",
    "import arvestapi\n",
    "from jhutils.local_files import collect_files, read_json\n",
    "from jhutils.misc import print_progress_bar\n",
    "import os\n",
    "\n",
    "print(\"üëç Ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare your media\n",
    "In Arvest, it is possible to upload media from your machine, or link to media that already exists online. In both cases, we will need to get the **path** to your media files, and bring them together in a list. First, here is a list of online files (hosted at the [Library of Congress](https://www.loc.gov/collections/fsa-owi-color-photographs/about-this-collection/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLINE_MEDIA_FILES = [\n",
    "    \"https://tile.loc.gov/storage-services/service/pnp/fsac/1a34000/1a34600/1a34630v.jpg\",\n",
    "    \"https://tile.loc.gov/storage-services/service/pnp/fsac/1a34000/1a34200/1a34209v.jpg\",\n",
    "    \"https://tile.loc.gov/storage-services/service/pnp/fsac/1a33000/1a33800/1a33859v.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a list of files on our computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TO_UPLOAD = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", 'test-media')\n",
    "LOCAL_MEDIA_FILES = collect_files(FOLDER_TO_UPLOAD)\n",
    "print(f\"üîç Found {len(LOCAL_MEDIA_FILES)} files in {FOLDER_TO_UPLOAD}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Connect to Arvest\n",
    "Next, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our username and our password. We've saved ours to a json file in order to keep things private."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIN_DATA = os.path.join(os.getcwd(), \"login_private.json\")\n",
    "credentials = read_json(LOGIN_DATA)\n",
    "\n",
    "ar = arvestapi.Arvest(credentials[\"email\"], credentials[\"password\"])\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the media to Arvest using the `add_media()` function. This will take one kwarg, `path`, which is the path to the file we'd like to upload. This can be a local path or an online path, the API package will take care of things for us. Each time, we'll grab the added media, and also modify the **title** and **description**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_media = []\n",
    "count = 0\n",
    "print(\"Uploading files...\")\n",
    "\n",
    "for i, media_file_path in enumerate(ONLINE_MEDIA_FILES):\n",
    "    print_progress_bar(i, len(ONLINE_MEDIA_FILES) - 1, f\"(Online file {i + 1}/{len(ONLINE_MEDIA_FILES)})\")\n",
    "\n",
    "    added_media = ar.add_media(path = media_file_path)\n",
    "    added_media.update_title(f\"Batch upload file {i + 1} (online)\")\n",
    "    added_media.update_description(f\"Uploaded to demonstrate batch media uploading from a python notebook.\")\n",
    "    uploaded_media.append(added_media)\n",
    "    count = count + 1\n",
    "\n",
    "for i, media_file_path in enumerate(LOCAL_MEDIA_FILES):\n",
    "    print_progress_bar(i, len(LOCAL_MEDIA_FILES) - 1, f\"(Local file {i + 1}/{len(LOCAL_MEDIA_FILES)})\")\n",
    "\n",
    "    added_media = ar.add_media(path = media_file_path)\n",
    "    added_media.update_title(f\"Batch upload file {i + 1} (local)\")\n",
    "    added_media.update_description(f\"Uploaded to demonstrate batch media uploading from a python notebook.\")\n",
    "    uploaded_media.append(added_media)\n",
    "    count = count + 1\n",
    "\n",
    "print(f\"üëè Added {count} media files to Arvest!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Update metadata\n",
    "Finally we can update our media's metadata. Among other things, this will notably be useful for parsing our documents and making sure that we find the files we need when scripting. We can deal with our metadata as a `dict` in python which we get using the `get_metadata()` function. We can then update this dict and use the `update_metadata()` function to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updating metadata...\")\n",
    "\n",
    "for i, added_media in enumerate(uploaded_media):\n",
    "    print_progress_bar(i, len(uploaded_media) - 1, f\"(File {i + 1}/{len(uploaded_media)})\")\n",
    "    media_metadata = added_media.get_metadata()\n",
    "    media_metadata[\"creator\"] = \"Batch media upload example script\"\n",
    "    media_metadata[\"identifier\"] = \"&&BATCH_UPLOAD\"\n",
    "    added_media.update_metadata(media_metadata)\n",
    "\n",
    "print(f\"üëç Metadata updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Batch remove media\n",
    "If we need to remove media files we can do so by parsing through all of our media and checjign certain conditions. For example, we can get all of our media files using the `get_medias()` function, then check it's metadata. If it's one of the files we want to remove, we can then use the `remove()` function.\n",
    "\n",
    "**Warning: there's no going back after using the remove function, so be careful!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_media = ar.get_medias()\n",
    "count = 0\n",
    "print(\"Removing files...\")\n",
    "\n",
    "for i, media_file in enumerate(all_media):\n",
    "    print_progress_bar(i, len(all_media) - 1, f\"(Processing file {i + 1}/{len(all_media)})\")\n",
    "    media_metadata = media_file.get_metadata()\n",
    "    if media_metadata[\"creator\"] == \"Batch media upload example script\" and media_metadata[\"identifier\"] == \"&&BATCH_UPLOAD\":\n",
    "        media_file.remove()\n",
    "        count = count + 1\n",
    "\n",
    "print(f\"üóëÔ∏è Removed {count} media files!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
