{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image embedding projection](https://raw.githubusercontent.com/arvest-data-in-context/ml-notebooks/refs/heads/main/docs/images/notebooks/video-shot-decomposition.png)\n",
    "\n",
    "In this video, we shall find a video that we have stored on Arvest, and then use the [Distant Viewing Toolkit](https://github.com/distant-viewing/dvt) to detect different shot changes in the video. Once this is done, we shall take the results and build an interactive IIIF Manifest which can be diirectly viewed in [Arvest](https://arvest.app)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Let's begin by installing and importing all of the different components we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing and importing packages...\")\n",
    "\n",
    "# Uninstall and reinstall packages for a clean environment\n",
    "!pip uninstall -q -y arvestapi\n",
    "!pip uninstall -q -y arvesttools\n",
    "!pip uninstall -q -y jhutils\n",
    "!pip uninstall -q -y iiif_prezi3\n",
    "!pip uninstall -q -y dvt\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/arvest-data-in-context/arvest-api.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/arvest-data-in-context/arvest-api-tools.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/jdchart/jh-py-utils.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/iiif-prezi/iiif-prezi3.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/distant-viewing/dvt.git\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# Import packages\n",
    "import arvestapi\n",
    "import arvesttools.manifest_creation\n",
    "from jhutils.local_files import read_json, write_json\n",
    "import jhutils.online_files\n",
    "from jhutils.misc import print_progress_bar_colab, slugify\n",
    "import os\n",
    "import dvt\n",
    "import iiif_prezi3\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import mimetypes\n",
    "mimetypes.add_type('image/webp', '.webp')\n",
    "\n",
    "TEMP_FOLDER = os.path.join(os.getcwd(), \"_TEMP\")\n",
    "if os.path.isdir(TEMP_FOLDER) == False:\n",
    "    os.makedirs(TEMP_FOLDER)\n",
    "\n",
    "print(\"üëç Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find our video\n",
    "The first step is to get the video that we wish to process. We have ours stored on our Arvest account, and we have given its metadata `identifier` field the value `\"API-TUTORIAL-CONTENT-AUTO-SHOT-DECOMP\"`. This allows us to find our media using the [Arvest API](https://github.com/arvest-data-in-context/arvest-api).\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user email and our password which we will give to an instance of the `arvestapi.Arvest()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = \"my_email@something.com\"\n",
    "PASSWORD = \"myarvestpassword\"\n",
    "\n",
    "ar = arvestapi.Arvest(EMAIL, PASSWORD)\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get all of our media using the `get_medias()` function, and search until we find the right video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_media = []\n",
    "media_items = ar.get_medias()\n",
    "\n",
    "for media_item in media_items:\n",
    "    media_item_metadata = media_item.get_metadata()\n",
    "    if media_item_metadata[\"identifier\"] == \"API-TUTORIAL-CONTENT-AUTO-SHOT-DECOMP\":\n",
    "        found_media.append(media_item)\n",
    "\n",
    "video_item = found_media[0]\n",
    "\n",
    "print(f\"üîç Found {len(found_media)} media files corresponding to search criteria.\")\n",
    "print(f\"Treating first item: \\\"{video_item.title}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we shall have to download the correspoinding video in order to analyze it. To do this, we shall use our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_video_path = jhutils.online_files.download(video_item.get_full_url(), dir = TEMP_FOLDER)\n",
    "\n",
    "print(f\"üëç Video downloaded to {local_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perform Analysis\n",
    "Now that we have our video, we are ready to analyze! We shall use the [Distant Viewing Toolkit](https://github.com/distant-viewing/dvt)'s `AnnoShotBreaks()` class (ntoe that, if you're running this for the first time, the model will first need to download).\n",
    "\n",
    "Once we have the results in frames, we calculate the start and end of each shot in seconds using the video's frame rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DVT's shot detection model:\n",
    "anno_breaks = dvt.AnnoShotBreaks()\n",
    "\n",
    "print(\"Processing...\")\n",
    "# Run the analysis here:\n",
    "breaks = anno_breaks.run(local_video_path)\n",
    "\n",
    "# The results are returned in frames, here we parse the results so that we have them in seconds:\n",
    "result_parse = {\"shots\" : []}\n",
    "frame_rate = dvt.video_info(local_video_path)[\"fps\"]\n",
    "for i in range(len(breaks[\"scenes\"][\"start\"])):\n",
    "    result_parse[\"shots\"].append([breaks[\"scenes\"][\"start\"][i] / frame_rate, breaks[\"scenes\"][\"end\"][i] / frame_rate])\n",
    "\n",
    "print(f\"üëç Analysis complete! Found {len(result_parse['shots'])} shots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export to Arvest\n",
    "Finally, we shall export the results of our analysis to an interactive IIIF Manifest that can be opened in Arvest. Let's begin by creating the basic Manifest with the [arvesttools](https://github.com/arvest-data-in-context/arvest-api-tools) package's `media_to_manifest()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = arvesttools.manifest_creation.media_to_manifest(video_item)\n",
    "print(\"üëç Manifest created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add a new Canvas for each of the detected shots that crops the original video according to the shot start and end times.\n",
    "\n",
    "**‚ÑπÔ∏è This is currently not working, as the IIIF Presentation API does not allow us to supply a start time to a video resource. Hopefully this feature shall be implemented soon.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, shot in enumerate(result_parse[\"shots\"]):\n",
    "\n",
    "#     duration = shot[1] - shot[0]\n",
    "    \n",
    "#     shot_canvas = copy.copy(manifest.items[0])\n",
    "#     shot_ap = copy.copy(shot_canvas.items[0])\n",
    "#     shot_an = copy.copy(shot_ap.items[0])\n",
    "#     shot_body = copy.copy(shot_an.body)\n",
    "\n",
    "#     shot_canvas.label = {\"en\" : [f\"{video_item.title} (shot {i + 1})\"]}\n",
    "#     shot_canvas.duration = \"{:.4f}\".format(duration)\n",
    "    \n",
    "#     original_target = shot_an.target.split(\"&t=\")[0]\n",
    "#     shot_an.target = f\"{original_target}&t={shot[0]:.2f},{shot[1]:.2f}\"\n",
    "\n",
    "#     shot_body.duration = duration\n",
    "\n",
    "#     shot_an.body = shot_body\n",
    "#     shot_ap.items = [shot_an]\n",
    "#     shot_canvas.items = [shot_ap]\n",
    "\n",
    "#     arvesttools.manifest_creation.append_canvas_to_manifest(manifest, shot_canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add a timed annotation to the main Canvas, one for each shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding annotations...\")\n",
    "for i, shot in enumerate(result_parse[\"shots\"]):\n",
    "    print_progress_bar_colab(i + 1, len(result_parse[\"shots\"]), f\"(shot {i + 1}/{len(result_parse['shots'])})\")\n",
    "\n",
    "    arvesttools.manifest_creation.add_textual_annotation(\n",
    "        manifest,\n",
    "        text_content = f\"<p><strong>Shot {i + 1}</strong><br>(<em>{shot[0]:.2f}-{shot[1]:.2f}</em>)</p>\",\n",
    "        t = {\"start\" : \"{:.4f}\".format(shot[0]), \"end\" : \"{:.4f}\".format(shot[1])}\n",
    "    )\n",
    "\n",
    "print(\"üëç Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can upload the Manifest to Arvest. You can either go and find it in your [workspace](https://workspace.arvest.app/) or view it at the url given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "local_path = os.path.join(TEMP_FOLDER, f\"{slugify(video_item.title)}-shot-decomposition.json\")\n",
    "write_json(local_path, manifest.dict())\n",
    "\n",
    "# Upload Manifest:\n",
    "added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "added_manifest.update_title(f\"{video_item.title} (automatic shot decomposition)\")\n",
    "added_manifest.update_description(\"A Manifest annotated using a video shot detection model.\")\n",
    "if video_item.thumbnail_url != None:\n",
    "    added_manifest.update_thumbnail_url(video_item.thumbnail_url)\n",
    "\n",
    "# Update metadata:\n",
    "manifest_metadata = added_manifest.get_metadata()\n",
    "manifest_metadata[\"creator\"] = \"Video shot deocmposition tutorial\"\n",
    "manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-VIDEO-SHOT-DECOMP\"\n",
    "added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "print(f\"üëç Manifest created, view it here: {added_manifest.get_preview_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleanup\n",
    "To finish, lets clean up our mess! First, we can delete the temporary folder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_FOLDER)\n",
    "print(f\"üóëÔ∏è {TEMP_FOLDER} removed !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can remove from Arvest all of our created Manifest. We can get all of our Manifests by using the `get_manifests()` function, then check the metadata. If it's one of the files we want to remove, we can then use the `remove()` function.\n",
    "\n",
    "**‚ö†Ô∏è Warning: there's no going back after using the remove function, so be careful! To avoid accidential removal, we've added a `REMOVE` variable that need to be set to `True` for the code to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE = False\n",
    "\n",
    "if REMOVE:\n",
    "    all_manifests = ar.get_manifests()\n",
    "    count = 0\n",
    "    print(\"Removing manifests...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_manifests):\n",
    "        print_progress_bar_colab(i + 1, len(all_manifests), f\"(Processing file {i + 1}/{len(all_manifests)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Video shot deocmposition tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-VIDEO-SHOT-DECOMP\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    print(f\"üóëÔ∏è Removed {count} items!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
