{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image embedding projection](https://raw.githubusercontent.com/arvest-data-in-context/ml-notebooks/refs/heads/main/docs/images/notebooks/image-embedding-projection.png)\n",
    "\n",
    "In this notebook, we shall take a collection of images, perform an embedding analysis using the [Distant Viewing Toolkit](https://github.com/distant-viewing/dvt), and then project this data into a 2 dimensional space. This is then exported as an annotated IIIF Manifest that can be opened in [Arvest](https://arvest.app), rendering an interface very similar to [PixPlot](https://dhlab.yale.edu/projects/pixplot/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Let's begin by installing and importing all of the different components we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing and importing packages...\")\n",
    "\n",
    "# Uninstall and reinstall packages for a clean environment\n",
    "!pip uninstall -q -y arvestapi\n",
    "!pip uninstall -q -y arvesttools\n",
    "!pip uninstall -q -y jhutils\n",
    "!pip uninstall -q -y iiif_prezi3\n",
    "!pip uninstall -q -y dvt\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/arvest-data-in-context/arvest-api.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/arvest-data-in-context/arvest-api-tools.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/jdchart/jh-py-utils.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/iiif-prezi/iiif-prezi3.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/distant-viewing/dvt.git\n",
    "!pip install -q --disable-pip-version-check opencv-python\n",
    "!pip install -q --disable-pip-version-check scikit-learn\n",
    "!pip install -q --disable-pip-version-check matplotlib\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# Import packages\n",
    "import arvestapi\n",
    "import arvesttools.manifest_creation\n",
    "from jhutils.local_files import read_json, write_json, get_image_info\n",
    "import jhutils.online_files\n",
    "from jhutils.misc import print_progress_bar_colab, slugify\n",
    "import os\n",
    "import dvt\n",
    "import iiif_prezi3\n",
    "import shutil\n",
    "import requests\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import mimetypes\n",
    "mimetypes.add_type('image/webp', '.webp')\n",
    "\n",
    "TEMP_FOLDER = os.path.join(os.getcwd(), \"_TEMP\")\n",
    "if os.path.isdir(TEMP_FOLDER) == False:\n",
    "    os.makedirs(TEMP_FOLDER)\n",
    "\n",
    "print(\"👍 Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare images\n",
    "You will first need a corpus of images to process. Here we give the example of a collection of images hosted from a free dataset of flower photos which can be found [here](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_URL = \"https://raw.githubusercontent.com/arvest-data-in-context/ml-notebooks/refs/heads/main/test-corpora/random-flowers.json\"\n",
    "\n",
    "response = requests.get(CORPUS_URL)\n",
    "if response.status_code == 200:\n",
    "    corpus = json.loads(response.text)[\"images\"]\n",
    "\n",
    "print(f\"🌁🌄🏞️ Loaded a corpus of {len(corpus)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process embeddings\n",
    "Next we can use the distant viewing toolkit to map the images within an embedding space. The first time you use dvt it will download the model onto your computer. We'll save the embedding data as a **numpy file** (`.npy`) so that we don't have to run this step again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILE = os.path.join(os.getcwd(), \"embeddings.npy\")\n",
    "\n",
    "embedder = dvt.AnnoEmbed()\n",
    "\n",
    "for i, image_data in enumerate(corpus):\n",
    "    print_progress_bar_colab(i + 1, len(corpus), f\"Treating {os.path.basename(image_data['url'])}\")\n",
    "    jhutils.online_files.download(image_data[\"url\"], dir = TEMP_FOLDER)\n",
    "    image_as_np = cv2.imread(os.path.join(TEMP_FOLDER, os.path.basename(image_data[\"url\"])))\n",
    "    image_as_np = cv2.cvtColor(image_as_np, cv2.COLOR_BGR2RGB)\n",
    "    embedding = embedder.run(image_as_np)[\"embedding\"]\n",
    "    if i == 0:\n",
    "        embedding_list = embedding\n",
    "    else:\n",
    "        embedding_list = np.vstack((embedding_list, embedding))\n",
    "\n",
    "print(embedding_list)\n",
    "np.save(EMBEDDINGS_FILE, embedding_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality reduction and clustering\n",
    "Now that we have our embedding data, we can use dimensionality reduction to crunch all of these dimensions down into 2 so that they can be projected into a 2-dimensional space. To do this, we'll use an one of the following dimensionality reduction algorithms: [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), [PCA]() or [UMAP](). Note that we also do some pre- and post-processing, the full process is: standardisation -> dimensionality reduction -> normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"tsne\"\n",
    "\n",
    "embedding_list = np.load(\"embeddings.npy\")\n",
    "standardized = StandardScaler().fit_transform(embedding_list)\n",
    "\n",
    "\n",
    "if METHOD == \"tsne\":\n",
    "    tsne = TSNE(n_components = 2, perplexity = 50, learning_rate=200, n_iter=5000)\n",
    "    reduced = tsne.fit_transform(standardized)\n",
    "if METHOD == \"pca\":\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(standardized)\n",
    "# if METHOD == \"umap\":\n",
    "#     reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, metric='euclidean')\n",
    "#     reduced = reducer.fit_transform(standardized)\n",
    "\n",
    "normalized = MinMaxScaler((0, 1)).fit_transform(reduced)\n",
    "\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we like, we can visualize the data in a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = np.transpose(normalized)\n",
    "plt.scatter(transposed[0], transposed[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (optional)\n",
    "\n",
    "Next we could perform some clustering on this data using [K-Means](https://en.wikipedia.org/wiki/K-means_clustering). We won't be using this data for our visualisation, but it is something that could potentially be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters = NUM_CLUSTERS, random_state = 0, n_init = \"auto\")\n",
    "clusters = kmeans.fit(normalized).labels_\n",
    "\n",
    "# Create a random colour map for visualisation:\n",
    "colour_map = {}\n",
    "used = []\n",
    "for item in clusters:\n",
    "    if item not in used:\n",
    "        colour_map[str(item)] = (random.random(), random.random(), random.random())\n",
    "        used.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the clusters in a scatter plot like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = np.transpose(normalized)\n",
    "col = []\n",
    "for item in clusters:\n",
    "    col.append(colour_map[str(item)])\n",
    "\n",
    "plt.scatter(transposed[0], transposed[1], c = col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export to Arvest\n",
    "Finally, we shall export the results of our analysis to an image file, and create an annotated (and therefore interactive) IIIF Manifest that can be consulted in [Arvest](https://arvest.app/en). First, we shall create a high-res PNG file that projects the corresponding images into the 2D space of the dimensionality reduction. We shall also keep a track of the coordinates so that we can create our annotations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(os.getcwd(), \"visualization-image.png\")\n",
    "COORDINATES_PATH = os.path.join(os.getcwd(), \"visualization-coordinates.json\")\n",
    "\n",
    "WIDTH = 5000\n",
    "HEIGHT = 5000\n",
    "PADDING = 100\n",
    "IMAGE_ZOOM = 0.1\n",
    "\n",
    "def scale(val, old_min, old_max, new_min, new_max):\n",
    "    return new_min + (((val - old_min) * (new_max - new_min)) / (old_max - old_min))\n",
    "\n",
    "# Function for adding each image to the main image:\n",
    "def add_image(full_image, coordinates_list, image_url, coordinates):\n",
    "  img_path = os.path.join(TEMP_FOLDER, os.path.basename(image_url))\n",
    "  img_data = get_image_info(img_path)\n",
    "  this_img = Image.open(img_path)\n",
    "\n",
    "  w = int(img_data[\"width\"] * IMAGE_ZOOM)\n",
    "  h = int(img_data[\"height\"] * IMAGE_ZOOM)\n",
    "  x = int(scale(int(int(float(coordinates[0]) * WIDTH) - (w * 0.5)), 0, WIDTH, PADDING, WIDTH - (PADDING * 2)))\n",
    "  y = int(scale(int(int(float(coordinates[1]) * HEIGHT) - (h * 0.5)), 0, HEIGHT, PADDING, HEIGHT - (PADDING * 2)))\n",
    "\n",
    "  this_img = this_img.resize((w, h))\n",
    "  full_image.paste(this_img, (x, y))\n",
    "\n",
    "  coordinates_list.append({\"url\" : image_url, \"x\" : x, \"y\" : y, \"w\" : w, \"h\" : h})\n",
    "\n",
    "# Initialize image and coordinates\n",
    "full_image = Image.new('RGBA', (WIDTH, HEIGHT))\n",
    "coordinates = {\"images\" : []}\n",
    "\n",
    "# Add all of the images:\n",
    "for i, item in enumerate(normalized):\n",
    "  image_data = corpus[i]\n",
    "  print_progress_bar_colab(i, len(corpus) - 1, f\"Treating {os.path.basename(image_data['url'])}\")\n",
    "  add_image(full_image, coordinates[\"images\"], image_data['url'], item)\n",
    "\n",
    "full_image.save(IMAGE_PATH)\n",
    "write_json(COORDINATES_PATH, coordinates)\n",
    "\n",
    "print(\"🎨 Image created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Manifests\n",
    "Now we need to create our Manifests. In order to make the main visualization Manifest truly interactive, we shall also make a _Manifest for each of the images in our corpus_. This must be done first, as we will need the URLs of these Manifests when creating our annotations.\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user email and our password which we will give to an instance of the `arvestapi.Arvest()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = \"my_email@something.com\"\n",
    "PASSWORD = \"myarvestpassword\"\n",
    "\n",
    "ar = arvestapi.Arvest(EMAIL, PASSWORD)\n",
    "print(f\"👍 Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our Manifests using the [arvesttools](https://github.com/arvest-data-in-context/arvest-api-tools) package's helper function `media_to_manifest()`. We'll create a Manifest for each file in our corpus, and keep a track of the URLs which are created in the `MANIFEST_DICT` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANIFEST_DICT = {}\n",
    "\n",
    "for i, image_data in enumerate(corpus):\n",
    "  print_progress_bar_colab(i + 1, len(corpus), f\"Creating a Manifest for {os.path.basename(image_data['url'])}\")\n",
    "\n",
    "  img_path = os.path.join(TEMP_FOLDER, os.path.basename(image_data['url']))\n",
    "  img_filename = os.path.splitext(os.path.basename(image_data['url']))[0]\n",
    "  img_data = get_image_info(img_path)\n",
    "\n",
    "  # Create the iiif_prezi3.Manifest:\n",
    "  manifest = arvesttools.manifest_creation.media_to_manifest(img_path)\n",
    "\n",
    "  # Update the ID to be the online location of the image:\n",
    "  manifest.items[0].items[0].items[0].body.id = image_data['url']\n",
    "\n",
    "  # Save the Manifest to disk\n",
    "  local_path = os.path.join(TEMP_FOLDER, f\"{slugify(img_filename)}.json\")\n",
    "  write_json(local_path, manifest.dict())\n",
    "\n",
    "  # And upload to Arvest:\n",
    "  added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "  added_manifest.update_title(f\"{img_filename}\")\n",
    "  added_manifest.update_description(\"Local view of an image embedding projection.\")\n",
    "  \n",
    "  manifest_metadata = added_manifest.get_metadata()\n",
    "  manifest_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "  manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "  added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "  # Keep track of the urls that are created:\n",
    "  MANIFEST_DICT[image_data['url']] = added_manifest.get_full_url()\n",
    "\n",
    "print(\"👍 Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the main visualization Manifest. First, we need to upload the image we created of the projection to Arvest. For this, we'll use the `add_media()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_media = ar.add_media(path = IMAGE_PATH)\n",
    "added_media.update_title(\"Image collection projection\")\n",
    "added_media.update_description(\"A projection in 2D space of a collection of images.\")\n",
    "\n",
    "media_metadata = added_media.get_metadata()\n",
    "media_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "media_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "added_media.update_metadata(media_metadata)\n",
    "\n",
    "print(f\"👍 Media uploaded to Arvest at the following url: {added_media.get_full_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the Manifest. Again, we'll use the `media_to_manifest()` function, which in this case can also accept an Arvest media item. \n",
    "\n",
    "Once we've created the Manifest, we can add annotations to render it interactive. We'll add an annotation for each of the Manifests  created earlier using the `add_textual_annotation()` function with the corresponding Manifest url and spatial position and dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manifest:\n",
    "manifest = arvesttools.manifest_creation.media_to_manifest(added_media)\n",
    "\n",
    "# Add an annotation for each Manifest:\n",
    "for item in coordinates[\"images\"]:\n",
    "    image_url = item[\"url\"]\n",
    "    manifest_url = MANIFEST_DICT[image_url]\n",
    "    xywh = {\"x\" : item[\"x\"], \"y\" : item[\"y\"], \"w\" : item[\"w\"], \"h\" : item[\"h\"]}\n",
    "    \n",
    "    arvesttools.manifest_creation.add_textual_annotation(\n",
    "        manifest,\n",
    "        text_content = f\"<p>{os.path.basename(image_url)}</p>\",\n",
    "        xywh = xywh,\n",
    "        linked_manifest = manifest_url\n",
    "    )\n",
    "\n",
    "# Save to disk:\n",
    "local_path = os.path.join(TEMP_FOLDER, \"projection-manifest.json\")\n",
    "write_json(local_path, manifest.dict())\n",
    "\n",
    "# And upload to Arvest:\n",
    "added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "added_manifest.update_title(\"Image embedding projection\")\n",
    "added_manifest.update_description(\"Projection of a collection of images in 2-D space.\")\n",
    "\n",
    "manifest_metadata = added_manifest.get_metadata()\n",
    "manifest_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "print(f\"👍 Manifest uploaded to Arvest at the following url: {added_manifest.get_preview_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleanup\n",
    "To finish, lets clean up our mess! First, we can delete the temporary folder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_FOLDER)\n",
    "os.remove(IMAGE_PATH)\n",
    "os.remove(COORDINATES_PATH)\n",
    "print(f\"🗑️ {TEMP_FOLDER} removed !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can remove from Arvest all of our content. We can get all of our content by using the `get_manifests()` and `get_medias()` functions, then check the metadata. If it's one of the files we want to remove, we can then use the `remove()` function.\n",
    "\n",
    "**⚠️ Warning: there's no going back after using the remove function, so be careful! To avoid accidential removal, we've added a `REMOVE` variable that need to be set to `True` for the code to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE = False\n",
    "\n",
    "if REMOVE:\n",
    "    all_manifests = ar.get_manifests()\n",
    "    count = 0\n",
    "    print(\"Removing manifests...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_manifests):\n",
    "        print_progress_bar_colab(i + 1, len(all_manifests), f\"(Processing file {i + 1}/{len(all_manifests)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Image embedding projection tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-IMAGE-EMBEDDING\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    all_media = ar.get_medias()\n",
    "    print(\"Removing medias...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_media):\n",
    "        print_progress_bar_colab(i + 1, len(all_media), f\"(Processing file {i + 1}/{len(all_media)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Image embedding projection tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-IMAGE-EMBEDDING\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    print(f\"🗑️ Removed {count} items!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
