{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image embedding projection](https://raw.githubusercontent.com/arvest-data-in-context/ml-notebooks/refs/heads/main/docs/images/notebooks/automatic-speech-recognition.png)\n",
    "\n",
    "In this notebook, we shall find a video that we have stored on Arvest, and then extract the words that are spoken using a speech recognition model provided by [vosk](https://alphacephei.com/vosk/). Once this is done, we shall take the results and build an interactive IIIF Manifest which can be diirectly viewed in [Arvest](https://arvest.app)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Let's begin by installing and importing all of the different components we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing and importing packages...\")\n",
    "\n",
    "# Uninstall and reinstall packages for a clean environment\n",
    "!pip uninstall -q -y arvestapi\n",
    "!pip uninstall -q -y arvesttools\n",
    "!pip uninstall -q -y jhutils\n",
    "!pip uninstall -q -y iiif_prezi3\n",
    "!pip uninstall -q -y dvt\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/arvest-data-in-context/arvest-api.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/arvest-data-in-context/arvest-api-tools.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/jdchart/jh-py-utils.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/iiif-prezi/iiif-prezi3.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/distant-viewing/dvt.git\n",
    "!pip install -q --disable-pip-version-check vosk\n",
    "!pip install -q --disable-pip-version-check librosa\n",
    "!pip install -q --disable-pip-version-check scipy\n",
    "\n",
    "# Import packages\n",
    "import arvestapi\n",
    "import arvesttools.manifest_creation\n",
    "from jhutils.local_files import read_json, write_json\n",
    "import jhutils.online_files\n",
    "from jhutils.misc import print_progress_bar_colab, slugify\n",
    "import os\n",
    "import dvt\n",
    "import iiif_prezi3\n",
    "import shutil\n",
    "import numpy as np\n",
    "import vosk\n",
    "import librosa\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import wiener\n",
    "import json\n",
    "import mimetypes\n",
    "mimetypes.add_type('image/webp', '.webp')\n",
    "\n",
    "TEMP_FOLDER = os.path.join(os.getcwd(), \"_TEMP\")\n",
    "if os.path.isdir(TEMP_FOLDER) == False:\n",
    "    os.makedirs(TEMP_FOLDER)\n",
    "\n",
    "print(\"üëç Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get the vosk model we want to use. The model we use will depend on the language of the content we wish to analyse - [here is a list](https://alphacephei.com/vosk/models) of models that vosk have avaiable. Change the `MODEL_NAME` variable to the model you wish to download and use.\n",
    "\n",
    "The `MODEL_PATH` variable allows you to choose where you would like to save the model. If the folder doesn't exist, it will get created. Even if you have already downloaded the model run this cell anyway as it will let the rest of the notebook know where to find the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables if needed:\n",
    "MODEL_NAME = \"vosk-model-small-en-us-0.15\"\n",
    "MODEL_PATH = os.path.join(os.getcwd(), \"models\")\n",
    "\n",
    "# Create folder if needed:\n",
    "if os.path.isdir(MODEL_PATH) == False:\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "# Download model if it doesn't already exist:\n",
    "print(f\"Downloading model \\\"{MODEL_NAME}\\\" to {MODEL_PATH}...\")\n",
    "if os.path.isdir(os.path.join(MODEL_PATH, MODEL_NAME)) == False:\n",
    "    jhutils.online_files.download_zip(os.path.join(\"https://alphacephei.com/vosk/models\", MODEL_NAME) + \".zip\", MODEL_PATH)\n",
    "\n",
    "# Load the model:\n",
    "model = vosk.Model(os.path.join(MODEL_PATH, MODEL_NAME))\n",
    "\n",
    "print(f\"üëç {MODEL_NAME} model ready and downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find our video\n",
    "The first step is to get the video that we wish to process. We have ours stored on our Arvest account, and we have given its metadata `identifier` field the value `\"API-TUTORIAL-CONTENT-SPEECH-REC\"`. This allows us to find our media using the [Arvest API](https://github.com/arvest-data-in-context/arvest-api).\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user email and our password which we will give to an instance of the `arvestapi.Arvest()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = \"my_email@something.com\"\n",
    "PASSWORD = \"myarvestpassword\"\n",
    "\n",
    "ar = arvestapi.Arvest(EMAIL, PASSWORD)\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get all of our media using the `get_medias()` function, and search until we find the right video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_media = []\n",
    "media_items = ar.get_medias()\n",
    "\n",
    "for media_item in media_items:\n",
    "    media_item_metadata = media_item.get_metadata()\n",
    "    if media_item_metadata[\"identifier\"] == \"API-TUTORIAL-CONTENT-SPEECH-REC\":\n",
    "        found_media.append(media_item)\n",
    "\n",
    "video_item = found_media[0]\n",
    "\n",
    "print(f\"üîç Found {len(found_media)} media files corresponding to search criteria.\")\n",
    "print(f\"Treating first item: \\\"{video_item.title}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall have to download the correspoinding video in order to analyze it. To do this, we shall use our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_video_path = jhutils.online_files.download(video_item.get_full_url(), dir = TEMP_FOLDER)\n",
    "\n",
    "print(f\"üëç Video downloaded to {local_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's convert the video into a wave file with a sample rate of 16000 - this is the type of file that vosk will accept for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = os.path.join(TEMP_FOLDER, os.path.splitext(os.path.basename(local_video_path))[0] + '.wav')\n",
    "!ffmpeg -i \"{local_video_path}\" -ar {16000} -ac 1 \"{audio_file_path}\" > /dev/null 2>&1\n",
    "print(\"üëç Converted to audio!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Audio pre-processing\n",
    "There are a few other things to do in order to make sure that the file will work best with vosk. Here, we scale the audio to 16 bit depth, as well as perform some simple noise reduction and audio normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing \\\"{os.path.basename(audio_file_path)}\\\"...\")\n",
    "    \n",
    "# Load the audio file with librosa:\n",
    "audio_data, sample_rate = librosa.load(audio_file_path, sr = None)\n",
    "\n",
    "# Perform noise reduction and normalization:\n",
    "noise_reduction = wiener(audio_data)\n",
    "normalized = librosa.util.normalize(noise_reduction)\n",
    "\n",
    "# Scale to 16 bit depth for vosk:\n",
    "scaled = np.int16(normalized * 32767)\n",
    "\n",
    "# Output file:\n",
    "wavfile.write(audio_file_path, sample_rate, scaled)\n",
    "print(\"üëç Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform Analysis\n",
    "Now that we have our audio source, we can run the speech recognition model. We shall use an instance of vosk's `KaldiRecognizer` class to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running speech recognition...\")\n",
    "\n",
    "# Create the vosk recognizer:\n",
    "recognizer = vosk.KaldiRecognizer(model, 16000)\n",
    "recognizer.SetWords(True)\n",
    "\n",
    "# Open the audio file:\n",
    "with wave.open(os.path.join(\"media_for_analysis\", audio_file_path), 'rb') as wf:\n",
    "    audio_data = wf.readframes(wf.getnframes())\n",
    "\n",
    "# Run the model:\n",
    "recognizer.AcceptWaveform(audio_data)\n",
    "result = json.loads(recognizer.Result())[\"result\"]\n",
    "\n",
    "# Print the results:\n",
    "print(f\"üëç Analysis complete! Found {len(result)} words.\")\n",
    "\n",
    "full_string = \"\"\n",
    "for item in result:\n",
    "    full_string = f\"{full_string}{item['word']} \"\n",
    "print(f\"\\\"{full_string}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export to Arvest\n",
    "Finally, we shall export the results of our analysis to an interactive IIIF Manifest that can be opened in Arvest. Let's begin by creating the basic Manifest with the [arvesttools](https://github.com/arvest-data-in-context/arvest-api-tools) package's `media_to_manifest()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = arvesttools.manifest_creation.media_to_manifest(video_item)\n",
    "print(\"üëç Manifest created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add a timed annotation to the main Canvas, one for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding annotations...\")\n",
    "for i, word in enumerate(result):\n",
    "    print_progress_bar_colab(i + 1, len(result), f\"(word {i + 1}/{len(result)})\")\n",
    "\n",
    "    arvesttools.manifest_creation.add_textual_annotation(\n",
    "        manifest,\n",
    "        text_content = f\"<p><strong>{word['word']}</strong><br>Confidence: {word['conf']}<br>(<em>{word['start']}-{word['end']}</em>)</p>\",\n",
    "        t = {\"start\" : word['start'], \"end\" : word['end']}\n",
    "    )\n",
    "\n",
    "print(\"üëç Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can upload the Manifest to Arvest. You can either go and find it in your [workspace](https://workspace.arvest.app/) or view it at the url given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "local_path = os.path.join(TEMP_FOLDER, f\"{slugify(video_item.title)}-shot-decomposition.json\")\n",
    "write_json(local_path, manifest.dict())\n",
    "\n",
    "# Upload Manifest:\n",
    "added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "added_manifest.update_title(f\"{video_item.title} (automatic speech recognition)\")\n",
    "added_manifest.update_description(\"A Manifest annotated using an automatic speech recognition model.\")\n",
    "if video_item.thumbnail_url != None:\n",
    "    added_manifest.update_thumbnail_url(video_item.thumbnail_url)\n",
    "\n",
    "# Update metadata:\n",
    "manifest_metadata = added_manifest.get_metadata()\n",
    "manifest_metadata[\"creator\"] = \"Automatic speech recognition tutorial\"\n",
    "manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-AUTO-SPEECH-REC\"\n",
    "added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "print(f\"üëç Manifest created, view it here: {added_manifest.get_preview_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleanup\n",
    "To finish, lets clean up our mess! First, we can delete the temporary folder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_FOLDER)\n",
    "print(f\"üóëÔ∏è {TEMP_FOLDER} removed !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can remove from Arvest all of our created Manifest. We can get all of our Manifests by using the `get_manifests()` function, then check the metadata. If it's one of the files we want to remove, we can then use the `remove()` function.\n",
    "\n",
    "**‚ö†Ô∏è Warning: there's no going back after using the remove function, so be careful! To avoid accidential removal, we've added a `REMOVE` variable that need to be set to `True` for the code to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE = False\n",
    "\n",
    "if REMOVE:\n",
    "    all_manifests = ar.get_manifests()\n",
    "    count = 0\n",
    "    print(\"Removing manifests...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_manifests):\n",
    "        print_progress_bar_colab(i + 1, len(all_manifests), f\"(Processing file {i + 1}/{len(all_manifests)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Automatic speech recognition tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-AUTO-SPEECH-REC\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    print(f\"üóëÔ∏è Removed {count} items!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
